version: 2.1


jobs:

  linting:
    docker: 
      - image: python:3.7.3-stretch
    steps:
      - checkout

      - run:
          name: check directory content
          command: |
            ls -la
      # Download and cache dependencies
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "requirements.txt" }}
            # fallback to using the latest cache if no exact match is found
            - v1-dependencies-

      - run:
          name: install dependencies
          command: |
            python3 -m venv venv
            . venv/bin/activate
            cd src
            ls -la
            make install
            # Install hadolint
            wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v1.16.3/hadolint-Linux-x86_64 &&\
            chmod +x /bin/hadolint
            
            #apk add --update docker
      
      - save_cache:
          paths:
            - ./venv
          key: v1-dependencies-{{ checksum "requirements.txt" }}

      # run lint!
      - run:
          name: run lint
          command: |
            . venv/bin/activate
            cd src
            make lint
    
  build:
    environment:
      IMAGE_NAME: leoadams/capstone
    docker:
      - image: docker:latest
        auth:
          username: $DOCKERHUB_USERNAME
          password: $DOCKERHUB_PASSWORD
    steps:
      - checkout
      - setup_remote_docker
      - restore_cache:
          keys:
            - v1-{{ .Branch }}
          paths:
            - /caches/capstone.tar
      - run:
          name: Load Docker image layer cache
          command: |
            set +o pipefail
            docker load -i /caches/capstone.tar | true
      - run:
          name: Build application Docker image
          command: |
            ls -la
            # docker build --tag=capstone .
            #docker build --cache-from=capstone -t "${IMAGE_NAME}:1" .
            docker build --cache-from=capstone -t "${IMAGE_NAME}:${CIRCLE_SHA1}" . 
            docker image ls
            
      - run:
          name: Save Docker image layer cache
          command: |
            mkdir -p /caches
            docker save -o /caches/capstone.tar capstone
      - save_cache:
          key: v1-{{ .Branch }}-{{ epoch }}
          paths:
            - /caches/capstone.tar

      - deploy:
          name: Push application Docker image
          command: |
            docker image ls
            dockerpath=leoadams/capstone
            echo "Docker ID and Image: $dockerpath"
            echo "$DOCKERHUB_ACCESS_TOKEN" | docker login -u "$DOCKERHUB_USERNAME" --password-stdin
            #docker push "${IMAGE_NAME}:1"
            #docker push leoadams/capstone:latest
            docker push "${IMAGE_NAME}:${CIRCLE_SHA1}"
            # if [ "$ {CIRCLE_BRANCH}" == "master" ]; then
            #   login="$(aws ecr get-login)"
            #   ${login}
            #   docker tag app "${ECR_ENDPOINT}/app:${CIRCLE_SHA1}"
            #   docker push "${ECR_ENDPOINT}/app:${CIRCLE_SHA1}"
            # fi



  deploy-infrastructure:
    environment:
      IMAGE_NAME: leoadams/capstone
    docker:
      # Docker image here that supports AWS CLI
      - image: amazon/aws-cli
    steps:
      # Checkout code from git
      - checkout
      - run:
          name: Install dependencies
          command: |
            yum -y install tar gzip jq
      
      - run:
          name: Ensure backend infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/vpc.yaml \
              --tags project=capstone \
              --stack-name capstone-eks-stack
        
      - run: 
          name: create k8s cluster
          command: | 
              subnets=$(aws cloudformation list-exports --query "Exports[?Name==\`SubnetIds\`].Value" --no-paginate --output text)
              secgroups=$(aws cloudformation list-exports --query "Exports[?Name==\`SecurityGroups\`].Value" --no-paginate --output text)
              echo $subnets
              echo $secgroups

              # create role for EKS
              # role_arn=$(aws iam create-role --profile capstone --role-name capstone-eks-role-nodes --assume-role-policy-document file://assume-policy.json | jq .Role.Arn | sed s/\"//g)
              # aws iam attach-role-policy --role-name capstone-eks-role --policy-arn  arn:aws:iam::aws:policy/AmazonEKSClusterPolicy

              #role_arn=$(aws iam get-role --role-name capstone-eks-role | jq .Role.Arn | sed s/\"//g)
              eks_role=$(aws iam list-roles | jq '.Roles' | jq '.[] | select(.RoleName=="capstone-eks-role")')

              if [[ -n $eks_role ]]; then
                echo "Role already exist"
                role_arn=$(aws iam get-role --role-name capstone-eks-role | jq .Role.Arn | sed s/\"//g)
              else
                role_arn=$(aws iam create-role --role-name capstone-eks-role --assume-role-policy-document file://assume-policy.json | jq .Role.Arn | sed s/\"//g)
                aws iam attach-role-policy --role-name capstone-eks-role --policy-arn  arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
              fi

              #cluster=$(aws eks describe-cluster --name capstone-eks | jq .cluster)

              #cluster=$(aws eks list-clusters | jq -c '.[] | select(.name=="capstone-eks")' | jq .name)
              cluster=$(aws eks list-clusters | jq '.clusters' | jq 'contains(["capstone-eks"])')

              #if [[ -n $cluster ]]; then
              if [[ $cluster == true ]]; then
                echo "Cluster already exist"
              else
                aws eks create-cluster \
                --name capstone-eks \
                --role-arn $role_arn \
                --resources-vpc-config subnetIds=$subnets,securityGroupIds=$secgroups,endpointPublicAccess=true,endpointPrivateAccess=false
                
              fi

              aws eks list-clusters

              #aws eks describe-cluster --name capstone-eks
              #role_nodes_arn=$(aws iam get-role --role-name capstone-eks-role-nodes | jq .Role.Arn | sed s/\"//g)
              #role_arn=$(aws iam create-role --role-name capstone-eks-role-nodes --assume-role-policy-document file://assume-node-policy.json | jq .Role.Arn | sed s/\"//g)

              node_role=$(aws iam list-roles | jq '.Roles' | jq '.[] | select(.RoleName=="capstone-eks-role-nodes")')

              if [[ -n $node_role ]]; then
                echo "Role already exist"
                role_nodes_arn=$(aws iam get-role --role-name capstone-eks-role-nodes | jq .Role.Arn | sed s/\"//g)
              else
                role_nodes_arn=$(aws iam create-role --role-name capstone-eks-role-nodes --assume-role-policy-document file://assume-node-policy.json | jq .Role.Arn | sed s/\"//g)
                aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
                aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
                aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
              fi
              
              
              
              subnet1=(${subnets//,/ }[0])

              #node1=$(aws eks describe-nodegroup --nodegroup-name node1 --cluster-name capstone-eks | jq .nodegroup.nodegroupName | sed s/\"//g)
              node1=$(aws eks list-nodegroups --cluster-name capstone-eks | jq '.nodegroups' | jq 'contains(["node1"])')

              #if [[ "$node1" == "node1" ]]; then
              if [[ "$node1" == true ]]; then
                echo "node group already exist"
              else
                aws eks create-nodegroup \
                --cluster-name capstone-eks \
                --nodegroup-name node1 \
                --node-role $role_nodes_arn \
                --subnets $subnet1 \
                --disk-size 200 \
                --scaling-config minSize=1,maxSize=2,desiredSize=1 \
                --instance-types t2.small
              fi

              aws eks update-kubeconfig --name capstone-eks --region us-west-2

              curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
              chmod +x ./kubectl && mv ./kubectl /usr/local/bin/kubectl

              #kubectl get deploy capstone-deploy -o=yaml | sed -e "s/$BlueVersion/${{CF_SHORT_REVISION}}/g" | kubectl apply --namespace=${NAMESPACE} -f - #Update Service YAML with Green version
              #kubectl patch deploy capstone-deploy -p "{\"spec\":{\"template\": {\"spec\": {\"template\": \"containers[0]\": \"${VERSION}\"}}}}"

              kubectl get service
              export BlueVersion=$(kubectl get service capstone-service -o=jsonpath='{.spec.selector.version}')
              echo $BlueVersion

              # if [[ -n $BlueVersion ]]; then 
              #   echo "show"; 
              # else 
              #   echo "missing"; 
              # fi

              kubectl get deployment capstone-deploy-${BlueVersion} -o=yaml | sed -e "s/$BlueVersion/${CIRCLE_SHA1}/g" | kubectl apply -f - #Deploy new version

              kubectl rollout status deployment/capstone-deploy-${CIRCLE_SHA1}

              URL=$(kubectl get service capstone-service -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')

              page=$(curl ${url}:8080)

              if [[ -n $page ]]
              then
                kubectl get service capstone-service -o=yaml | sed -e "s/$BlueVersion/${CIRCLE_SHA1}/g" | kubectl apply -f -
                kubectl delete deployment capstone-deploy-$BlueVersion #Delete blue version
                return 0
              else
                return 1
              fi


              # if [[ -n $role_nodes_arn ]]; then
              #   echo "Role already exist"
              # else
              #   role_nodes_arn=$(aws iam create-role --role-name capstone-eks-role-nodes --assume-role-policy-document file://assume-node-policy.json | jq .Role.Arn | sed s/\"//g)
              # fi

              # create role for nodes
              
              # role_arn=$(aws iam create-role --role-name capstone-eks-role-nodes --assume-role-policy-document file://assume-node-policy.json | jq .Role.Arn | sed s/\"//g)
              # aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
              # aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
              # aws iam attach-role-policy --role-name capstone-eks-role-nodes --policy-arn  arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly

      


      # - run:
      #     name: Ensure backend infrastructure exists
      #     command: |
      #       aws cloudformation deploy \
      #         --template-file .circleci/files/backend.yml \
      #         --tags project=udapeople \
      #         --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
      #         --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
          # exit 1
      # - run:
      #     name: Ensure frontend infrastructure exist
      #     command: |
      #       aws cloudformation deploy \
      #         --template-file .circleci/files/frontend.yml \
      #         --tags project=udapeople \
      #         --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
      #         --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}" 
               
          # exit 1
#       - run:
#           name: Add back-end ip to ansible inventory
#           command: |
#             # Your code here
#             aws ec2 describe-instances \
#               --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
#               --query "Reservations[*].Instances[*].PublicIpAddress" \
#               --output text >> .circleci/ansible/inventory.txt
#             cat .circleci/ansible/inventory.txt
#             # exit 1
#       - persist_to_workspace:
#           root: ~/
#           paths:
#             - project/.circleci/ansible/inventory.txt
# #       # Here's where you will add some code to rollback on failure
#       - destroy-environment:
#           workflow_id: ${CIRCLE_WORKFLOW_ID:0:7}
#       - slack-fail-notification
         
workflows:
  default:
    jobs:
      - linting
      - build:
          requires: [linting]    
      - deploy-infrastructure:
          requires: [build]